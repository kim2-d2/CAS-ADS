{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 3, Module 1, Data and Data Management, CAS Applied Data Science, 2020-08-21, S. Haug, University of Bern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data acquisition on the world wide web\n",
    "\n",
    "**Learning outcomes:**\n",
    "\n",
    "Participants will be able to collect data from www sources. Examples are provided and exercised. We have about 1.5h hours for this tutorial.\n",
    "\n",
    "**Table of Contents**\n",
    "- 3.1 Read json from the web\n",
    "- 3.2 Retrieve and display pictures and files from the web\n",
    "- 3.3 Scraping webpages (html scraping)\n",
    "- 3.4 Cron jobs and Scheduled tasks\n",
    "- Some notes and links concerning social media\n",
    "\n",
    "**Further sources**\n",
    "- Examples all over internet\n",
    "- A book: https://www.packtpub.com/big-data-and-business-intelligence/mastering-social-media-mining-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Analyse Aare with data from https://aareguru.existenz.ch/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from website, bring it into a format which can be imported into a dataframe, plot the time series and the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "startlink = \"https://aareguru.existenz.ch/v2018/current\"\n",
    "f = urlopen(startlink)\n",
    "l = str(f.read())\n",
    "l=l.split('[')[3].split(']')\n",
    "l='['+l[0]+']'\n",
    "#print(l)\n",
    "df = pd.read_json(l)\n",
    "df.tail()\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the timestamp as index for plotting\n",
    "dft = df.copy()\n",
    "dft = dft.set_index('timestamp')\n",
    "dft.tail()\n",
    "#dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dft['temperature'].plot()\n",
    "plt.show()\n",
    "dft['flow'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exercise \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible further exercise or project for Module 1 and 2**\n",
    "\n",
    "Find some colleague who can get the historical data (knows how to use the API) out of https://aareguru.existenz.ch/. Bring all data into one data frame. Look for correlations, averages (per month, per year ...). Combine the data with weather data, e.g. the wind on the Thun lake. For the Model 2 project, try to make a linear regression model predicting the Aare temperature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Get pictures (or files) from webpages\n",
    "\n",
    "Get 10 pictures from a webserver with the Image module and show it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as image\n",
    "from IPython.display import display\n",
    "\n",
    "for i in range(1,10): \n",
    "    url = 'https://fl-5-232.zhdk.cloud.switch.ch/evd.planeview.00001.00000000'+str(i)+'.png'\n",
    "    img = image(url)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a file into the current directory (for further processing). In this exmple we downloud a file with tweets from the present US president. Then we load it into a dataframe for some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "fname=\"https://fl-5-232.zhdk.cloud.switch.ch/t-tweets.txt\"\n",
    "# retrieve that file and save it as t-tweets.txt\n",
    "urllib.request.urlretrieve(fname,'t-tweets.txt')\n",
    "#It has a json format, so we import it with the read_json method\n",
    "df = pd.read_json('t-tweets.txt')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how often Obama was mentioned in these 245 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "txt = df['text']\n",
    "for i in range(0,len(txt)):\n",
    "    if 'Obama' in txt[i]: counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Yes, these are prefiltered tweets about Obama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Scrape Webpages (html scraping)\n",
    "\n",
    "There are several billion online websites. With python you can easily read and parse this data if you have the links. Since pages are linked, one can in principle unnest probably all internet for webpages. \n",
    "\n",
    "In Python there is a library https://www.crummy.com/software/BeautifulSoup/bs4/doc/ for pulling data out of html and xml pages. We don't practise that library here, however, if you at some point deal with a lot of html, you may want to use it.\n",
    "\n",
    "Here we get the links on the landing page of Science IT Suport, and if there are any, the mail addresses. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "\n",
    "#startlink = \"http://www.scits.unibe.ch\"\n",
    "startlink = \"http://www.scits.unibe.ch/about_us/contact/\"\n",
    "f = urlopen(startlink)\n",
    "myfile = f.read()\n",
    "lines = str(myfile).split(' ')\n",
    "links = []\n",
    "addresses = []\n",
    "for line in lines:\n",
    "    if 'http' in line:\n",
    "        links.append(line)\n",
    "    elif '@' in line:\n",
    "        #print(line)\n",
    "        addresses.append(line)\n",
    "df_links = pd.DataFrame(links,columns=['Links'])\n",
    "df_addrs = pd.DataFrame(addresses,columns=['Adresses'])\n",
    "df_addrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is not optimal as you have probably seen. Lets use regular expressions instead (from StackOverflow). Regular expressions are a bit geeky, but very powerful and great fun. If you don't wan't to learn them, you mostly find the expression you want by googling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # the regular expression module\n",
    "startlink = \"http://www.scits.unibe.ch/about_us/contact/\"\n",
    "f = urlopen(startlink)\n",
    "html = f.read()\n",
    "# Extract email addresses\n",
    "reobj = re.compile(r\"\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,6}\\b\", re.IGNORECASE)\n",
    "print(re.findall(reobj, html.decode('utf-8')))\n",
    "# Extract urls (links)\n",
    "urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', html.decode('utf-8'))\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables from webpages\n",
    "\n",
    "If you or someone else pubslishes data in html tables, it can be collected with pandas quite easily, actually directly without using the urllib module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\n",
    "tables = pd.read_html(link)\n",
    "df = tables[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which countries are the people consuming the most energy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df = df.iloc[1:,1:2] \n",
    "s_df['consum'] = df.iloc[1:,7].astype('float')\n",
    "s_df.sort_values('consum', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df['consum'].plot(kind='hist',bins=50,range=(1000,50000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Run cron jobs (linux) / scheduled tasks (windows)\n",
    "\n",
    "If you need to collect data on a regular basis you typically run a so-called cron job on a Linux machine or a scheduled task on a Windows machine. For example, you can specify when and how often your Python data acquisition script should run. Maybe you would like to collect new Aare data every day. Of course, your computer has to be running and connected to internet at the scheduled time.\n",
    "\n",
    "Here an example collecting Aare data. You can creae a python script aarecollect.py with a text editor. The code can be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import datetime\n",
    "link = \"https://aareguru.existenz.ch/v2018/current\"\n",
    "filename = 'aare-data-'+ str(datetime.date.today()) +'.json'\n",
    "urllib.request.urlretrieve(link,filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run this script from the command line by typing \"python aarecollect.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Linux you specify when to execute this file and how often in the so called crontab. On Windows by configuring a Scheduled Task. Google instructions if you need to do this. For example, this file can be added to a users crontab by typing \"crontab ~/tmp/mycron.sh\" on the command line, mycron.sh being this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">cat ~/tmp/mycron.sh\n",
    "# Run aarecollect every day at 1am by default\n",
    "0 1 * * * cd ~/tmp && ./python aarecollect.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible exercise\n",
    "\n",
    "Hier a nice little challenge for you. Use the code above (together with a for loop or two) to scrape all webpages of your employer company for public available email addresses and put them into a dataframe :) (of course you could scrape all internet now, but we don't go that far today)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write, copy and paste your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notes and links concerning social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Google search\n",
    "\n",
    "There is are APIs for doing google searches from Python. Hier is one explained. \n",
    "\n",
    "https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Twitter\n",
    "\n",
    "Twitter generates about 500M tweets per day. Thus, data mining on twitter can be interesting.\n",
    "\n",
    "Note: there are rate limits in the use of the Twitter API, as well as limitations in case you want to provide a downloadable data-set, see:\n",
    "\n",
    "https://dev.twitter.com/overview/terms/agreement-and-policy\n",
    "\n",
    "https://dev.twitter.com/rest/public/rate-limiting\n",
    "\n",
    "Tweepy is one python module with clients for thwe Twitter API. \n",
    "\n",
    "- https://www.tweepy.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instagram\n",
    "\n",
    "Largest photo sharing social media platform with 500 million monthly active users, and 95 million pictures and videos uploaded on Instagram daily in 2018 (?). \n",
    "\n",
    "https://stackoverflow.com/questions/61010431/how-to-start-with-the-instagramapi-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
